This strong baseline uses a fine-tuned BERT (bert-base-uncased) model to perform hate speech classification. Unlike the majority-class baseline, this model leverages contextual language representations learned during pre-training and adapts them to our dataset through supervised fine-tuning. All text is tokenized using BERT’s tokenizer with truncation and max-length (512 tokens) padding for consistency across samples.
To address class imbalance in the training data, we apply a class-weighted cross-entropy loss, where weights are computed from the inverse label frequencies. We implement this custom loss by subclassing Hugging Face’s Trainer. The model is fine-tuned for three epochs with a batch size of 32 and a learning rate of 5e-5, and is evaluated on the  F1 score for the hate speech class.
After training, we evaluate the model on the held-out test set. This strong baseline substantially outperforms the majority classifier and establishes a meaningful performance threshold for more advanced models.
