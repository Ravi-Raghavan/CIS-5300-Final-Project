{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad82945",
   "metadata": {},
   "source": [
    "# Test Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c49ddc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "78.81711885571573% of our dataset has label = 0 and 21.182881144284256% of our dataset has label = 1\n",
      "Class Weights: tensor([0.2118, 0.7882], device='mps:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating...: 100%|██████████████████████████████████████████████| 104/104 [00:39<00:00,  2.63it/s]\n"
     ]
    }
   ],
   "source": [
    "# Import system tooling\n",
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "from itertools import product\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "os.environ[\"USE_TF\"] = \"0\"\n",
    "\n",
    "# Get path of Milestone #2 and Milestone #3\n",
    "Milestone_2 = \"../Milestone #2\"\n",
    "Milestone_3 = \"../Milestone #3\"\n",
    "sys.path.append(Milestone_2)\n",
    "sys.path.append(Milestone_3)\n",
    "\n",
    "# Pandas, Numpy, Torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from scipy.stats import mode\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "## Set Random State for Reproducability\n",
    "random_state = 42\n",
    "\n",
    "# Import Hugging Face Tooling\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import BertModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from datasets import Dataset\n",
    "from transformers import Trainer\n",
    "\n",
    "# Load Safe Tensors\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "# Use CPU/MPS if possible\n",
    "device = None\n",
    "if \"google.colab\" in sys.modules:\n",
    "    # Running in Colab\n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "else:\n",
    "    # Not in Colab (e.g., Mac)\n",
    "    device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "train_df = pd.read_csv('../data/train_data.csv')\n",
    "dev_df = pd.read_csv('../data/dev_data.csv')\n",
    "test_df = pd.read_csv('../data/test_data.csv')\n",
    "\n",
    "# Compute Class Proportions\n",
    "p0 = (train_df['label'] == 0).mean() # Computes the percentage of our training dataset that has label = 0\n",
    "p1 = (train_df['label'] == 1).mean() # Computes the percentage of our training dataset that has label = 1\n",
    "print(f\"{p0  * 100}% of our dataset has label = 0 and {p1  * 100}% of our dataset has label = 1\")\n",
    "\n",
    "# Define Custom Loss Criterion to Address Class Imbalance\n",
    "class_weights = torch.tensor([p1, p0]).float().to(device)\n",
    "custom_criterion = nn.CrossEntropyLoss(weight = class_weights)\n",
    "print(f\"Class Weights: {class_weights}\")\n",
    "\n",
    "# Fetch BERT Tokenizer from HuggingFace\n",
    "bert_model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "\n",
    "# Define BERT + CNN Hybrid Model\n",
    "class BertCNNClassifier(nn.Module):\n",
    "    def __init__(self, bert_model_name=\"bert-base-uncased\", num_labels=2, dropout=0.3):\n",
    "        super().__init__()\n",
    "\n",
    "        # BERT Encoder\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name) # Fetch BERT Encoder\n",
    "        hidden_size = self.bert.config.hidden_size # Dimensionality of the encoder layers and the pooler layer\n",
    "\n",
    "        # Define Convolutional Layers\n",
    "        self.conv1 = nn.Conv1d(\n",
    "            in_channels=hidden_size,\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Conv1d(\n",
    "            in_channels=256,\n",
    "            out_channels=256,\n",
    "            kernel_size=3,\n",
    "            padding=1\n",
    "        )\n",
    "\n",
    "        # Define ReLU and Dropout \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Add Dense Layers\n",
    "        self.fc1 = nn.Linear(256, 128)\n",
    "        self.fc2 = nn.Linear(128, num_labels)\n",
    "\n",
    "    # Define Forward Pass\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
    "        # Fetch sequence output from BERT Encoder\n",
    "        sequence_output = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # Fetch output from last hidden state\n",
    "        x = sequence_output.last_hidden_state # Shape: (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Transpose x prior to convolutional layers\n",
    "        x = x.permute(0, 2, 1) # Shape: (batch, hidden_size [Represents Channels], seq_len)\n",
    "\n",
    "        # CNN + ReLU + Dropout\n",
    "        x = self.conv1(x) # Shape: (Batch, 256, Output Sequence Length_1)\n",
    "        x = self.relu(x) # Shape: (Batch, 256, Output Sequence Length_1)\n",
    "        x = self.dropout(x) # Shape: (Batch, 256, Output Sequence Length_1)\n",
    "\n",
    "        # CNN + ReLU\n",
    "        x = self.conv2(x) # Shape: (Batch, 256, Output Sequence Length_2)\n",
    "        x = self.relu(x) # Shape: (Batch, 256, Output Sequence Length_2)\n",
    "\n",
    "        # Perform Global Max Pooling by taking the maximum across the sequence dimension for each channel\n",
    "        x, _ = torch.max(x, dim = 2) # Shape: (Batch, 256)\n",
    "\n",
    "        # Run through Dense + ReLU + Dropout + Dense\n",
    "        x = self.fc1(x) # Shape: (Batch, 128)\n",
    "        x = self.relu(x) # Shape: (Batch, 128)\n",
    "        x = self.dropout(x) # Shape: (Batch, 128)\n",
    "        logits = self.fc2(x) # Shape: (Batch, 2)\n",
    "\n",
    "        # Return model output\n",
    "        return SequenceClassifierOutput(logits=logits)\n",
    "\n",
    "class BertLSTMClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model_name=\"bert-base-uncased\",\n",
    "        num_labels=2,\n",
    "        hidden_dim=256,\n",
    "        num_layers=2,\n",
    "        bidirectional=True,\n",
    "        dropout=0.3\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Load pretrained BERT encoder\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        bert_hidden = self.bert.config.hidden_size\n",
    "\n",
    "        # LSTM configuration\n",
    "        self.bidirectional = bidirectional\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=bert_hidden,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "\n",
    "        lstm_output_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "\n",
    "        # Fully connected classifier layers\n",
    "        self.fc1 = nn.Linear(lstm_output_dim, 128)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids=None, labels=None):\n",
    "        # BERT encoder\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            return_dict=True\n",
    "        )\n",
    "\n",
    "        # Extract sequence embeddings\n",
    "        x = outputs.last_hidden_state  # (batch, seq_len, hidden_size)\n",
    "\n",
    "        # Feed into LSTM\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)     # (batch, seq_len, hidden_dim * num_directions)\n",
    "\n",
    "        if self.bidirectional:\n",
    "            # Take the last hidden state of LSTM for forward and backward directions\n",
    "            x = torch.cat((h_n[-2], h_n[-1]), dim=1)\n",
    "        else:\n",
    "            # Take the last hidden state of LSTM\n",
    "            x = h_n[-1]\n",
    "\n",
    "        # Classifier layers\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        return SequenceClassifierOutput(logits=logits)\n",
    "\n",
    "# Define load model\n",
    "def load_model(model_class, model_path, device=\"mps\"):\n",
    "    device = torch.device(device) # Define device\n",
    "    model = None # Define model\n",
    "\n",
    "    if \"Milestone2\" in model_path:\n",
    "        model = BertForSequenceClassification.from_pretrained(model_path, num_labels = 2)\n",
    "    else:\n",
    "        model = model_class()\n",
    "        state_dict = load_file(os.path.join(model_path, \"model.safetensors\"))\n",
    "        model.load_state_dict(state_dict)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Define Model paths\n",
    "BERT_LSTM_path = os.path.join(Milestone_3, \"Milestone3-BERT-BiLSTM-FinalModel\")\n",
    "BERT_CNN_path = os.path.join(Milestone_3, \"Milestone3-BERT-CNN-FinalModel\")\n",
    "BERT_path = os.path.join(Milestone_2, \"Milestone2-Baseline-BERT-FinalModel\")\n",
    "\n",
    "# Define Models and create ensemble\n",
    "BERT_LSTM = load_model(BertLSTMClassifier, BERT_LSTM_path)\n",
    "BERT_CNN = load_model(BertCNNClassifier, BERT_CNN_path)\n",
    "BERT = load_model(None, BERT_path)\n",
    "ensemble = [BERT_LSTM, BERT_CNN, BERT]\n",
    "\n",
    "# Create Text Dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        # Fetch Text and Label\n",
    "        text = row['text'].to_list()\n",
    "        label = torch.tensor(row['label'].to_list(), dtype=torch.long)\n",
    "\n",
    "        enc = self.tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length = self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": enc[\"input_ids\"],\n",
    "            \"attention_mask\": enc[\"attention_mask\"],\n",
    "            \"labels\": label,\n",
    "        }\n",
    "\n",
    "# Create Datasets and DataLoaders\n",
    "batch_size = 32\n",
    "train_dataset = TextDataset(train_df, tokenizer)\n",
    "dev_dataset  = TextDataset(dev_df, tokenizer)\n",
    "test_dataset = TextDataset(test_df, tokenizer)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\n",
    "dev_loader  = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define function to get single model output\n",
    "def get_single_model_output(model, dataloader):\n",
    "    all_preds = [] # Store All Predictions\n",
    "    all_labels = [] # Store All Labels\n",
    "\n",
    "    # Set up progress bar\n",
    "    pbar = tqdm.tqdm(total=len(dataloader), desc=\"Evaluating...\", ncols=100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Get Input IDs, Attention Mask, and Labels\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device) # Shape: (Batch Size, )\n",
    "\n",
    "            # get predictions from the model\n",
    "            out = model(input_ids=input_ids, attention_mask=attention_mask) # Get Model Output\n",
    "            logits = out.logits\n",
    "            preds = torch.argmax(logits, dim=1) # Shape: (Batch Size, )\n",
    "\n",
    "            # Store predictions + labels\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# Define Hard Voting Function\n",
    "def hard_vote_predict(ensemble, dataloader):\n",
    "    all_preds = [] # Store All Predictions\n",
    "    all_labels = [] # Store All Labels\n",
    "\n",
    "    # Set up progress bar\n",
    "    pbar = tqdm.tqdm(total=len(dataloader), desc=\"Evaluating...\", ncols=100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Get Input IDs, Attention Mask, and Labels\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device) # Shape: (Batch Size, )\n",
    "\n",
    "            # Store predictions per model\n",
    "            per_model_preds = []\n",
    "\n",
    "            # get predictions from each model\n",
    "            for m in ensemble:\n",
    "                out = m(input_ids=input_ids, attention_mask=attention_mask) # Get Model Output\n",
    "                logits = out.logits\n",
    "                preds = torch.argmax(logits, dim=1) # Shape: (Batch Size, )\n",
    "                per_model_preds.append(preds.cpu().numpy())\n",
    "\n",
    "            # stack shape: (ensemble size, batch_size)\n",
    "            stacked = np.vstack(per_model_preds)\n",
    "\n",
    "            # majority vote\n",
    "            final_preds = mode(stacked, axis = 0).mode.tolist() # Shape: (Batch Size, )\n",
    "\n",
    "            # Store predictions + labels\n",
    "            all_preds.extend(final_preds)\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# Define Maximum Voting Function\n",
    "def max_vote_predict(ensemble, dataloader):\n",
    "    all_logits = [] # Store All Logits\n",
    "    all_preds = [] # Store All Predictions\n",
    "    all_labels = [] # Store All Labels\n",
    "\n",
    "    # Set up progress bar\n",
    "    pbar = tqdm.tqdm(total=len(dataloader), desc=\"Evaluating...\", ncols=100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Get Input IDs, Attention Mask, and Labels\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device) # Shape: (Batch Size, )\n",
    "\n",
    "            # Store logits per model\n",
    "            per_model_logits = []\n",
    "\n",
    "            # get predictions from each model\n",
    "            for m in ensemble:\n",
    "                out = m(input_ids=input_ids, attention_mask=attention_mask) # Get Model Output\n",
    "                logits = out.logits\n",
    "                per_model_logits.append(logits.cpu().numpy()) # Shape: (Batch Size, Number of Logits)\n",
    "\n",
    "            # stack shape: (Batch Size, Number of Logits, # of Models in Ensemble)\n",
    "            stacked = np.stack(per_model_logits, axis = -1)\n",
    "\n",
    "            # Max Vote\n",
    "            max_probs = np.max(stacked, axis = -1) # Shape: (Batch Size, Number of Logits)\n",
    "            final_preds = np.argmax(max_probs, axis = -1).tolist() # Shape: (Batch Size, )\n",
    "\n",
    "            # Store predictions + labels\n",
    "            all_preds.extend(final_preds)\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# Define Soft Voting Function\n",
    "def soft_vote_predict(ensemble, dataloader):\n",
    "    all_logits = [] # Store All Logits\n",
    "    all_preds = [] # Store All Predictions\n",
    "    all_labels = [] # Store All Labels\n",
    "\n",
    "    # Set up progress bar\n",
    "    pbar = tqdm.tqdm(total=len(dataloader), desc=\"Evaluating...\", ncols=100)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            # Get Input IDs, Attention Mask, and Labels\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device) # Shape: (Batch Size, )\n",
    "\n",
    "            # Store logits per model\n",
    "            per_model_logits = []\n",
    "\n",
    "            # get predictions from each model\n",
    "            for m in ensemble:\n",
    "                out = m(input_ids=input_ids, attention_mask=attention_mask) # Get Model Output\n",
    "                logits = out.logits\n",
    "                per_model_logits.append(logits.cpu().numpy()) # Shape: (Batch Size, Number of Logits)\n",
    "\n",
    "            # stack shape: (Batch Size, Number of Logits, # of Models in Ensemble)\n",
    "            stacked = np.stack(per_model_logits, axis = -1)\n",
    "\n",
    "            # Soft Vote\n",
    "            avg_probs = np.mean(stacked, axis = -1) # Shape: (Batch Size, Number of Logits)\n",
    "            final_preds = np.argmax(avg_probs, axis = -1).tolist() # Shape: (Batch Size, )\n",
    "\n",
    "            # Store predictions + labels\n",
    "            all_preds.extend(final_preds)\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "            # Update progress bar\n",
    "            pbar.update(1)\n",
    "\n",
    "    pbar.close()\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "# Define Method for stacking\n",
    "def stacking(ensemble, dataloader):\n",
    "    # Define function to fetch logits from all datasets\n",
    "    def fetch_logits(ensemble, dataloader):\n",
    "        all_logits = [] # Store All Logits\n",
    "        all_labels = [] # Store All Labels\n",
    "\n",
    "        # Set up progress bar\n",
    "        pbar = tqdm.tqdm(total=len(dataloader), desc=\"Fetching Logits...\", ncols=100)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in dataloader:\n",
    "                # Get Input IDs, Attention Mask\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"labels\"].to(device)\n",
    "\n",
    "                # Store logits per model\n",
    "                per_model_logits = []\n",
    "\n",
    "                # get predictions from each model\n",
    "                for m in ensemble:\n",
    "                    out = m(input_ids=input_ids, attention_mask=attention_mask) # Get Model Output\n",
    "                    logits = out.logits\n",
    "                    per_model_logits.append(logits.cpu().numpy())\n",
    "\n",
    "                # stack shape: (batch_size, # of logits * ensemble size) and store in all_logits\n",
    "                stacked = np.hstack(per_model_logits)\n",
    "                all_logits.append(stacked)\n",
    "                all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "                # Update progress bar\n",
    "                pbar.update(1)\n",
    "\n",
    "        pbar.close()\n",
    "        return np.concatenate(all_logits, axis = 0), np.array(all_labels)\n",
    "\n",
    "    logits_cache = {\n",
    "        \"train\": \"train_logits.npy\",\n",
    "        \"dev\":   \"dev_logits.npy\",\n",
    "        \"test\":  \"test_logits.npy\",\n",
    "    }\n",
    "\n",
    "    train_logits_path = logits_cache['train']\n",
    "    dev_logits_path = logits_cache['dev']\n",
    "    test_logits_path = logits_cache['test']\n",
    "\n",
    "    if os.path.exists(train_logits_path):\n",
    "        train_logits = np.load(train_logits_path)\n",
    "        train_labels = np.load(\"train_labels.npy\")\n",
    "    else:\n",
    "        train_logits, train_labels = fetch_logits(ensemble, train_loader)\n",
    "        np.save(train_logits_path, train_logits)\n",
    "        np.save(\"train_labels.npy\", train_labels)\n",
    "\n",
    "    if os.path.exists(dev_logits_path):\n",
    "        dev_logits = np.load(dev_logits_path)\n",
    "        dev_labels = np.load(\"dev_labels.npy\")\n",
    "    else:\n",
    "        dev_logits, dev_labels = fetch_logits(ensemble, dev_loader)\n",
    "        np.save(dev_logits_path, dev_logits)\n",
    "        np.save(\"dev_labels.npy\", dev_labels)\n",
    "\n",
    "    if os.path.exists(test_logits_path):\n",
    "        test_logits = np.load(test_logits_path)\n",
    "        test_labels = np.load(\"test_labels.npy\")\n",
    "    else:\n",
    "        test_logits, test_labels = fetch_logits(ensemble, test_loader)\n",
    "        np.save(test_logits_path, test_logits)\n",
    "        np.save(\"test_labels.npy\", test_labels)\n",
    "\n",
    "    print(f\"Compute Shape of train_logits: {train_logits.shape}, Shape of Train Labels: {train_labels.shape}\")\n",
    "    print(f\"Compute Shape of dev_logits: {dev_logits.shape}, Shape of Dev Labels: {dev_labels.shape}\")\n",
    "    print(f\"Compute Shape of test_logits: {test_logits.shape}, Shape of Test Labels: {test_labels.shape}\")\n",
    "\n",
    "    # Create Logistic Regression Meta Learner\n",
    "    param_grid = {\n",
    "        \"C\": [0.01, 0.1, 1, 10],           # regularization strength\n",
    "        \"penalty\": [\"l1\", \"l2\"],                \n",
    "        \"max_iter\": [200],\n",
    "        \"class_weight\":[{0: p1, 1: p0}]\n",
    "    }\n",
    "\n",
    "    log_reg = LogisticRegression() # Create Model\n",
    "\n",
    "    # Store best param set\n",
    "    best_params = None\n",
    "    best_dev_f1 = 0\n",
    "    best_model = None\n",
    "\n",
    "    # Create list of param names + all combinations\n",
    "    keys = list(param_grid.keys())\n",
    "    values = list(param_grid.values())\n",
    "\n",
    "    # Iterate through the param grid\n",
    "    for combo in product(*values):\n",
    "        params = dict(zip(keys, combo))\n",
    "\n",
    "        if params[\"penalty\"] == \"l1\":\n",
    "            solver = \"liblinear\"\n",
    "        else:\n",
    "            solver = \"lbfgs\"\n",
    "\n",
    "        model = LogisticRegression(\n",
    "            C=params[\"C\"],\n",
    "            penalty=params[\"penalty\"],\n",
    "            max_iter=params['max_iter'],\n",
    "            solver=solver,\n",
    "            class_weight=params[\"class_weight\"],\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        model.fit(train_logits, train_labels)\n",
    "\n",
    "        # Evaluate on Dev Set\n",
    "        dev_pred = model.predict(dev_logits)\n",
    "        dev_f1 = f1_score(dev_labels, dev_pred)\n",
    "\n",
    "        # Print Results\n",
    "        print(f\"Parameter Set: {params}, Dev Results: {dev_f1}\")\n",
    "\n",
    "        # Store best params\n",
    "        if dev_f1 > best_dev_f1:\n",
    "            best_dev_f1 = dev_f1\n",
    "            best_params = {**params}\n",
    "            best_model = model\n",
    "    \n",
    "    # Get Predictions from best_model\n",
    "    best_model_predictions = best_model.predict(test_logits)\n",
    "    return best_model_predictions, test_labels\n",
    "    \n",
    "# Evaluate and Save Results\n",
    "def generate_output_and_ground_truth(ensemble_method, ensemble, dataloader):\n",
    "    if ensemble_method == \"Hard Vote\":\n",
    "        y_pred, y_true = hard_vote_predict(ensemble, dataloader)\n",
    "    elif ensemble_method == \"Soft Vote\":\n",
    "        y_pred, y_true = soft_vote_predict(ensemble, dataloader)\n",
    "    elif ensemble_method == \"Max Vote\":\n",
    "        y_pred, y_true = max_vote_predict(ensemble, dataloader)\n",
    "    elif ensemble_method == \"Stacking\":\n",
    "        y_pred, y_true = stacking(ensemble, dataloader)\n",
    "    elif ensemble_method == \"BERT\":\n",
    "        y_pred, y_true = get_single_model_output(BERT, dataloader)\n",
    "    elif ensemble_method == \"BERT-LSTM\":\n",
    "        y_pred, y_true = get_single_model_output(BERT_LSTM, dataloader)\n",
    "    elif ensemble_method == \"BERT-CNN\":\n",
    "        y_pred, y_true = get_single_model_output(BERT_CNN, dataloader)\n",
    "\n",
    "    text = test_df['text'].tolist()\n",
    "    return text, y_pred, y_true\n",
    "\n",
    "text, y_pred, y_true = generate_output_and_ground_truth(\n",
    "        \"BERT-LSTM\",\n",
    "        ensemble,\n",
    "        test_loader\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1897ef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48f6c152c7b24934b7649ef8a9378cc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/26427 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6dbd8964a046018166ecf9fc806ad4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3303 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46d592d97b554316a0bdf4e364659f9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3304 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_hf_dataset = Dataset.from_pandas(train_df)\n",
    "dev_hf_dataset = Dataset.from_pandas(dev_df)\n",
    "test_hf_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "# Tokenize Text Data\n",
    "def tokenize_function(row):\n",
    "    tokens = tokenizer(row['text'], truncation = True, padding = 'max_length', max_length = tokenizer.model_max_length)\n",
    "    row['input_ids'] = tokens['input_ids']\n",
    "    row['attention_mask'] = tokens['attention_mask']\n",
    "    row['token_type_ids'] = tokens['token_type_ids']\n",
    "    return row\n",
    "\n",
    "train_hf_dataset = train_hf_dataset.map(tokenize_function)\n",
    "dev_hf_dataset = dev_hf_dataset.map(tokenize_function)\n",
    "test_hf_dataset = test_hf_dataset.map(tokenize_function)\n",
    "\n",
    "print(\"Going to initialize Trainer\")\n",
    "hf_trainer = Trainer(model=BERT_LSTM, train_dataset=train_hf_dataset,\n",
    "    eval_dataset=dev_hf_dataset)\n",
    "print(\"Already initialized Trainer\")\n",
    "\n",
    "# Use Trainer.predict to get predictions on the HF dataset\n",
    "preds_output = hf_trainer.predict(test_hf_dataset)\n",
    "logits = preds_output.predictions  # raw logits\n",
    "y_true = preds_output.label_ids     # true labels\n",
    "y_pred_hf = logits.argmax(axis=1)      # predicted labels\n",
    "type(y_pred_hf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
